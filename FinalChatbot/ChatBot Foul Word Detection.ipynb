{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatBot - Foul Word Detection\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Chatbots, or conversational interfaces as they are also known, present a new way for individuals to interact with computer systems. Traditionally, to get a question answered by a software program involved using a search engine, or filling out a form. A chatbot allows a user to simply ask questions in the same manner that they would address a human. The most well known chatbots currently are voice chatbots: Alexa and Siri. However, chatbots are currently being adopted at a high rate on computer chat platforms.\n",
    "\n",
    "The technology at the core of the rise of the chatbot is natural language processing (“NLP”). Recent advances in machine learning have greatly improved the accuracy and effectiveness of natural language processing, making chatbots a viable option for many organizations. This improvement in NLP is firing a great deal of additional research which should lead to continued improvement in the effectiveness of chatbots in the years to come.\n",
    "\n",
    "We will be evaluating different chatbots first and creating a chatbot. We are evaluating based on hit and miss.Goal is to integrate any website with a chatbot. Objective is that it is domain specific for now, can be extended to be scalable across other platforms. We will use RASA NLU to understand the questions in a correct manner and also take care of foul language being used. We will use intent classification and entity extraction. \n",
    "\n",
    "Foul Word detection can be detected using a profanity filter which is what is implemented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Rasa NLU?\n",
    "\n",
    "Rasa NLU is an open source NLP library for intent classification and entity extraction. You can think of it as a set of high-level APIs for building your own language parser using existing NLP and ML libraries.\n",
    "\n",
    "## Why Rasa NLU?\n",
    "\n",
    "1. We don’t have to hand over all your training data to Google, Microsoft, Amazon, or Facebook.\n",
    "2. Machine Learning is not one-size-fits all. You can tweak and customize models for your training data.\n",
    "3. Rasa NLU runs wherever we want, so we don’t have to make an extra network request for every message that comes in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install rasa_core;\n",
    "import logging, io, json, warnings\n",
    "logging.basicConfig(level=\"INFO\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a Story for chatbot. This is domain specific.\n",
    "A story starts with ## and you can give it a name. lines that start with * are messages sent by the user. Although you don't write the actual message, but rather the intent (and the entities) that represent what the user means. If you don't know about intents and entities, don't worry! We will talk about them more later. Lines that start with - are actions taken by your bot. In this case all of our actions are just messages sent back to the user, like utter_greet, but in general an action can do anything, including calling an API and interacting with the outside world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stories are for a dialogue flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'stories_md' (str) to file 'stories.md'.\n"
     ]
    }
   ],
   "source": [
    "stories_md = \"\"\"\n",
    "## happy path               <!-- name of the story - just for debugging -->\n",
    "* greet              \n",
    "  - utter_greet\n",
    "* mood_great               <!-- user utterance, in format intent[entities] -->\n",
    "  - utter_happy\n",
    "* mood_affirm\n",
    "  - utter_happy\n",
    "* mood_affirm\n",
    "  - utter_goodbye\n",
    "  \n",
    "  \n",
    "## sad path 1               <!-- this is already the start of the next story -->\n",
    "* greet\n",
    "  - utter_greet             <!-- action the bot should execute -->\n",
    "* mood_unhappy\n",
    "  - utter_cheer_up\n",
    "  - utter_did_that_help\n",
    "* mood_affirm\n",
    "  - utter_happy\n",
    "  \n",
    "  \n",
    "\n",
    "## sad path 2\n",
    "* greet\n",
    "  - utter_greet\n",
    "* mood_unhappy\n",
    "  - utter_cheer_up\n",
    "  - utter_did_that_help\n",
    "* mood_deny\n",
    "  - utter_goodbye\n",
    "  \n",
    "## strange user\n",
    "* mood_affirm\n",
    "  - utter_happy\n",
    "* mood_affirm\n",
    "  - utter_unclear\n",
    "\n",
    "## say goodbye\n",
    "* goodbye\n",
    "  - utter_goodbye\n",
    "  - utter_goodday\n",
    "\n",
    "## no foul\n",
    "* foul\n",
    "  - utter_foul\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "%store stories_md > stories.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a sample list.We should list all of the intents and actions that show up in your stories. This is also the place to write templates, which contain the messages your bot can send back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We define the intents in this file for intent classsification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'domain_yml' (str) to file 'domain.yml'.\n"
     ]
    }
   ],
   "source": [
    "domain_yml = \"\"\"\n",
    "intents:\n",
    "  - greet\n",
    "  - goodbye\n",
    "  - mood_affirm\n",
    "  - mood_deny\n",
    "  - mood_great\n",
    "  - mood_unhappy\n",
    "  - foul\n",
    "\n",
    "actions:\n",
    "- utter_greet\n",
    "- utter_cheer_up\n",
    "- utter_did_that_help\n",
    "- utter_happy\n",
    "- utter_goodbye\n",
    "- utter_unclear\n",
    "- utter_goodday\n",
    "- utter_foul\n",
    "\n",
    "templates:\n",
    "  utter_greet:\n",
    "  - text: \"Hey! How are you?\"\n",
    "\n",
    "  utter_cheer_up:\n",
    "  - text: \"Here is something to cheer you up:\"\n",
    "    image: \"https://i.imgur.com/nGF1K8f.jpg\"\n",
    "\n",
    "  utter_did_that_help:\n",
    "  - text: \"Did that help you?\"\n",
    "\n",
    "  utter_unclear:\n",
    "  - text: \"I am not sure what you are aiming for.\"\n",
    "  \n",
    "  utter_happy:\n",
    "  - text: \"Great carry on!\"\n",
    "\n",
    "  utter_goodbye:\n",
    "  - text: \"Bye\"\n",
    "  \n",
    "  utter_goodday:\n",
    "  - text: \"See you soon\"\n",
    "  \n",
    "  utter_foul:\n",
    "  - text: \"Please do not use such language\"\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "%store domain_yml > domain.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_spacy = \"\"\"\n",
    "{\n",
    "  \"pipeline\":\"spacy_sklearn\",\n",
    "  \"path\":\"./models/nlu\",\n",
    "  \"data\":\"./data/data.json\"\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use Keras. It is a neural networks library written in Python that is high-level in nature – which makes it extremely simple and intuitive to use.We will also use Agent class provides a convenient interface for the most important Rasa Core functionality.It includes training, handling messages, loading a dialogue model, getting the next action, and handling a channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Processed Story Blocks: 100%|█████████████████████████████████████████████| 6/6 [00:00<00:00, 288.32it/s, # trackers=1]\n",
      "Processed Story Blocks: 100%|█████████████████████████████████████████████| 6/6 [00:00<00:00, 229.55it/s, # trackers=6]\n",
      "Processed Story Blocks: 100%|████████████████████████████████████████████| 6/6 [00:00<00:00, 187.19it/s, # trackers=12]\n",
      "Processed Story Blocks: 100%|████████████████████████████████████████████| 6/6 [00:00<00:00, 140.73it/s, # trackers=13]\n",
      "Processed trackers: 100%|█████████████████████████████████████████████| 176/176 [00:08<00:00, 19.55it/s, # actions=183]\n",
      "Processed actions: 183it [00:00, 293.95it/s, # examples=183]\n",
      "Processed trackers: 100%|█████████████████████████████████████████████| 176/176 [00:07<00:00, 23.57it/s, # actions=183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_1 (Masking)          (None, 5, 17)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                6400      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 6,730\n",
      "Trainable params: 6,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "183/183 [==============================] - ETA: 5s - loss: 2.3565 - acc: 0.0000e+0 - 1s 7ms/step - loss: 2.3238 - acc: 0.0984\n",
      "Epoch 2/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 2.2933 - acc: 0.156 - 0s 208us/step - loss: 2.2698 - acc: 0.2022\n",
      "Epoch 3/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 2.2130 - acc: 0.281 - 0s 255us/step - loss: 2.2297 - acc: 0.2240\n",
      "Epoch 4/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 2.2321 - acc: 0.156 - 0s 220us/step - loss: 2.1953 - acc: 0.2350\n",
      "Epoch 5/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 2.1794 - acc: 0.281 - 0s 234us/step - loss: 2.1543 - acc: 0.3333\n",
      "Epoch 6/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 2.1641 - acc: 0.281 - 0s 223us/step - loss: 2.1096 - acc: 0.3333\n",
      "Epoch 7/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 2.0850 - acc: 0.281 - 0s 252us/step - loss: 2.0709 - acc: 0.3443\n",
      "Epoch 8/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 2.0364 - acc: 0.375 - 0s 243us/step - loss: 2.0263 - acc: 0.3552\n",
      "Epoch 9/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.9793 - acc: 0.375 - 0s 241us/step - loss: 1.9768 - acc: 0.3607\n",
      "Epoch 10/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.9134 - acc: 0.312 - 0s 267us/step - loss: 1.9354 - acc: 0.3552\n",
      "Epoch 11/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.8182 - acc: 0.468 - 0s 218us/step - loss: 1.8969 - acc: 0.3607\n",
      "Epoch 12/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.9782 - acc: 0.343 - 0s 251us/step - loss: 1.8664 - acc: 0.3607\n",
      "Epoch 13/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.6811 - acc: 0.562 - 0s 242us/step - loss: 1.8487 - acc: 0.3607\n",
      "Epoch 14/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.9135 - acc: 0.343 - 0s 218us/step - loss: 1.8357 - acc: 0.3607\n",
      "Epoch 15/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.6581 - acc: 0.500 - 0s 256us/step - loss: 1.7989 - acc: 0.3607\n",
      "Epoch 16/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.7827 - acc: 0.312 - 0s 223us/step - loss: 1.7827 - acc: 0.3607\n",
      "Epoch 17/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.9380 - acc: 0.281 - 0s 245us/step - loss: 1.7894 - acc: 0.3607\n",
      "Epoch 18/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.7571 - acc: 0.375 - 0s 267us/step - loss: 1.7525 - acc: 0.3607\n",
      "Epoch 19/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.9809 - acc: 0.218 - 0s 210us/step - loss: 1.7459 - acc: 0.3607\n",
      "Epoch 20/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.5209 - acc: 0.500 - 0s 272us/step - loss: 1.7142 - acc: 0.3607\n",
      "Epoch 21/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.9093 - acc: 0.218 - 0s 225us/step - loss: 1.7152 - acc: 0.3607\n",
      "Epoch 22/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.9214 - acc: 0.187 - 0s 256us/step - loss: 1.6908 - acc: 0.3607\n",
      "Epoch 23/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.5966 - acc: 0.437 - 0s 234us/step - loss: 1.6641 - acc: 0.3607\n",
      "Epoch 24/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.6808 - acc: 0.281 - 0s 207us/step - loss: 1.6398 - acc: 0.3607\n",
      "Epoch 25/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.4893 - acc: 0.500 - 0s 207us/step - loss: 1.6197 - acc: 0.3607\n",
      "Epoch 26/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.4773 - acc: 0.468 - 0s 223us/step - loss: 1.6068 - acc: 0.3661\n",
      "Epoch 27/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.5161 - acc: 0.406 - 0s 212us/step - loss: 1.5857 - acc: 0.3716\n",
      "Epoch 28/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.6080 - acc: 0.437 - 0s 201us/step - loss: 1.5726 - acc: 0.3716\n",
      "Epoch 29/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.5800 - acc: 0.343 - 0s 196us/step - loss: 1.5377 - acc: 0.3825\n",
      "Epoch 30/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.5329 - acc: 0.375 - 0s 232us/step - loss: 1.5199 - acc: 0.3770\n",
      "Epoch 31/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.3266 - acc: 0.500 - 0s 223us/step - loss: 1.4985 - acc: 0.3661\n",
      "Epoch 32/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.5421 - acc: 0.343 - 0s 215us/step - loss: 1.4742 - acc: 0.4153\n",
      "Epoch 33/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.3842 - acc: 0.437 - 0s 234us/step - loss: 1.4254 - acc: 0.4262\n",
      "Epoch 34/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.4393 - acc: 0.406 - 0s 213us/step - loss: 1.4300 - acc: 0.4153\n",
      "Epoch 35/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.2700 - acc: 0.562 - 0s 213us/step - loss: 1.3919 - acc: 0.4262\n",
      "Epoch 36/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.3482 - acc: 0.593 - 0s 196us/step - loss: 1.3480 - acc: 0.5191\n",
      "Epoch 37/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.4626 - acc: 0.375 - 0s 191us/step - loss: 1.3157 - acc: 0.5082\n",
      "Epoch 38/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.4375 - acc: 0.468 - 0s 214us/step - loss: 1.2912 - acc: 0.5246\n",
      "Epoch 39/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.2391 - acc: 0.593 - 0s 223us/step - loss: 1.2634 - acc: 0.5738\n",
      "Epoch 40/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.1574 - acc: 0.593 - 0s 218us/step - loss: 1.2370 - acc: 0.5792\n",
      "Epoch 41/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.1910 - acc: 0.687 - 0s 207us/step - loss: 1.2501 - acc: 0.6066\n",
      "Epoch 42/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.3036 - acc: 0.625 - 0s 221us/step - loss: 1.1991 - acc: 0.6503\n",
      "Epoch 43/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.2676 - acc: 0.593 - 0s 198us/step - loss: 1.1605 - acc: 0.7049\n",
      "Epoch 44/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.2472 - acc: 0.625 - 0s 200us/step - loss: 1.1487 - acc: 0.6940\n",
      "Epoch 45/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.0823 - acc: 0.781 - 0s 212us/step - loss: 1.0504 - acc: 0.7869\n",
      "Epoch 46/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.0789 - acc: 0.843 - 0s 213us/step - loss: 1.0348 - acc: 0.8033\n",
      "Epoch 47/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.8691 - acc: 0.843 - 0s 219us/step - loss: 1.0643 - acc: 0.7541\n",
      "Epoch 48/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.8867 - acc: 0.906 - 0s 233us/step - loss: 1.0012 - acc: 0.8033\n",
      "Epoch 49/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 1.1785 - acc: 0.687 - 0s 201us/step - loss: 1.0017 - acc: 0.8142\n",
      "Epoch 50/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.9612 - acc: 0.843 - 0s 218us/step - loss: 0.9501 - acc: 0.8087\n",
      "Epoch 51/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.9802 - acc: 0.718 - 0s 240us/step - loss: 0.9268 - acc: 0.8033\n",
      "Epoch 52/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.8138 - acc: 0.906 - 0s 240us/step - loss: 0.8865 - acc: 0.8634\n",
      "Epoch 53/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.9284 - acc: 0.906 - 0s 225us/step - loss: 0.8609 - acc: 0.8907\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - ETA: 0s - loss: 0.7281 - acc: 0.968 - 0s 207us/step - loss: 0.8479 - acc: 0.8743\n",
      "Epoch 55/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.8082 - acc: 0.843 - 0s 223us/step - loss: 0.8390 - acc: 0.8361\n",
      "Epoch 56/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.7066 - acc: 0.906 - 0s 202us/step - loss: 0.8175 - acc: 0.8525\n",
      "Epoch 57/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.9271 - acc: 0.875 - 0s 189us/step - loss: 0.8590 - acc: 0.8525\n",
      "Epoch 58/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.8682 - acc: 0.843 - 0s 245us/step - loss: 0.8092 - acc: 0.8142\n",
      "Epoch 59/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.7796 - acc: 0.843 - 0s 234us/step - loss: 0.7856 - acc: 0.8361\n",
      "Epoch 60/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.6585 - acc: 0.906 - 0s 223us/step - loss: 0.7369 - acc: 0.8907\n",
      "Epoch 61/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.7955 - acc: 0.875 - 0s 245us/step - loss: 0.7439 - acc: 0.8743\n",
      "Epoch 62/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.7554 - acc: 0.906 - 0s 253us/step - loss: 0.7099 - acc: 0.8798\n",
      "Epoch 63/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.6008 - acc: 0.968 - 0s 223us/step - loss: 0.6503 - acc: 0.9290\n",
      "Epoch 64/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.7146 - acc: 0.906 - 0s 245us/step - loss: 0.6607 - acc: 0.9071\n",
      "Epoch 65/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.7171 - acc: 0.875 - 0s 235us/step - loss: 0.6631 - acc: 0.9016\n",
      "Epoch 66/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.8516 - acc: 0.781 - 0s 272us/step - loss: 0.6403 - acc: 0.8907\n",
      "Epoch 67/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.6758 - acc: 0.937 - 0s 233us/step - loss: 0.6313 - acc: 0.9180\n",
      "Epoch 68/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.7440 - acc: 0.843 - 0s 278us/step - loss: 0.6490 - acc: 0.8907\n",
      "Epoch 69/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.6531 - acc: 0.906 - 0s 235us/step - loss: 0.6169 - acc: 0.9071\n",
      "Epoch 70/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.5152 - acc: 0.937 - 0s 256us/step - loss: 0.5964 - acc: 0.9016\n",
      "Epoch 71/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.6843 - acc: 0.906 - 0s 258us/step - loss: 0.6317 - acc: 0.8798\n",
      "Epoch 72/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.5353 - acc: 0.843 - 0s 238us/step - loss: 0.5789 - acc: 0.8743\n",
      "Epoch 73/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.6053 - acc: 0.812 - 0s 232us/step - loss: 0.5320 - acc: 0.9126\n",
      "Epoch 74/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.5859 - acc: 0.875 - 0s 255us/step - loss: 0.5500 - acc: 0.9071\n",
      "Epoch 75/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.4997 - acc: 0.906 - 0s 226us/step - loss: 0.5560 - acc: 0.8852\n",
      "Epoch 76/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.6067 - acc: 0.875 - 0s 223us/step - loss: 0.5127 - acc: 0.9290\n",
      "Epoch 77/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.5689 - acc: 0.937 - 0s 235us/step - loss: 0.5093 - acc: 0.9399\n",
      "Epoch 78/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.5217 - acc: 0.906 - 0s 227us/step - loss: 0.4729 - acc: 0.9290\n",
      "Epoch 79/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.3942 - acc: 0.937 - 0s 212us/step - loss: 0.4625 - acc: 0.9235\n",
      "Epoch 80/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.4483 - acc: 1.000 - 0s 221us/step - loss: 0.4674 - acc: 0.9454\n",
      "Epoch 81/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.3204 - acc: 0.968 - 0s 213us/step - loss: 0.4620 - acc: 0.9180\n",
      "Epoch 82/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.4567 - acc: 0.937 - 0s 209us/step - loss: 0.4934 - acc: 0.9016\n",
      "Epoch 83/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.3220 - acc: 0.968 - 0s 199us/step - loss: 0.4418 - acc: 0.9235\n",
      "Epoch 84/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.5090 - acc: 0.937 - 0s 190us/step - loss: 0.4405 - acc: 0.9454\n",
      "Epoch 85/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.6467 - acc: 0.781 - 0s 228us/step - loss: 0.4665 - acc: 0.9071\n",
      "Epoch 86/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.4878 - acc: 0.937 - 0s 225us/step - loss: 0.4496 - acc: 0.9235\n",
      "Epoch 87/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.5562 - acc: 0.875 - 0s 218us/step - loss: 0.4475 - acc: 0.9344\n",
      "Epoch 88/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.4251 - acc: 0.875 - 0s 202us/step - loss: 0.4068 - acc: 0.9126\n",
      "Epoch 89/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.3075 - acc: 0.906 - 0s 202us/step - loss: 0.3929 - acc: 0.9290\n",
      "Epoch 90/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.4000 - acc: 0.937 - 0s 229us/step - loss: 0.3782 - acc: 0.9126\n",
      "Epoch 91/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.3623 - acc: 0.937 - 0s 213us/step - loss: 0.3704 - acc: 0.9454\n",
      "Epoch 92/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.4369 - acc: 0.906 - 0s 205us/step - loss: 0.3587 - acc: 0.9399\n",
      "Epoch 93/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.5706 - acc: 0.906 - 0s 233us/step - loss: 0.3725 - acc: 0.9344\n",
      "Epoch 94/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.5049 - acc: 0.875 - 0s 199us/step - loss: 0.3773 - acc: 0.9235\n",
      "Epoch 95/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.3569 - acc: 0.875 - 0s 207us/step - loss: 0.3464 - acc: 0.9290\n",
      "Epoch 96/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.2857 - acc: 0.968 - 0s 218us/step - loss: 0.3237 - acc: 0.9563\n",
      "Epoch 97/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.3120 - acc: 0.968 - 0s 196us/step - loss: 0.3610 - acc: 0.9344\n",
      "Epoch 98/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.4202 - acc: 0.937 - 0s 196us/step - loss: 0.3032 - acc: 0.9617\n",
      "Epoch 99/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.3157 - acc: 0.937 - 0s 202us/step - loss: 0.3609 - acc: 0.9126\n",
      "Epoch 100/100\n",
      "183/183 [==============================] - ETA: 0s - loss: 0.3360 - acc: 0.937 - 0s 223us/step - loss: 0.3216 - acc: 0.9399\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import unicode_literals\n",
    "from rasa_core.policies.keras_policy import KerasPolicy\n",
    "from rasa_core.policies.memoization import MemoizationPolicy\n",
    "from rasa_core.agent import Agent\n",
    "from rasa_core.featurizers import (MaxHistoryTrackerFeaturizer, BinarySingleStateFeaturizer)\n",
    "\n",
    "featurizer = MaxHistoryTrackerFeaturizer(BinarySingleStateFeaturizer(), max_history=5)\n",
    "\n",
    "agent = Agent('domain.yml', policies=[MemoizationPolicy(max_history=5),KerasPolicy(featurizer)])\n",
    "                        \n",
    "agent.train(\n",
    "        'stories.md',\n",
    "        validation_split=0.0,\n",
    "        #max_history=3,\n",
    "        epochs=100\n",
    ");\n",
    "\n",
    "agent.persist('models/dialogue');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we just trained the dialogue model - so basically the conversational flow. So the bot will only understand structured input and no natural language yet. \n",
    "\n",
    "Go try it out with typing \"/\" + one of the intents from your domain before, e.g.:\n",
    "\n",
    "/greet\n",
    "\n",
    "/mood_affirm\n",
    "\n",
    "/mood_deny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type here and the chatbot detects foul words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prafu\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.20.2 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\prafu\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator LinearSVC from version 0.20.2 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\prafu\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.20.2 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\prafu\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator _SigmoidCalibration from version 0.20.2 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\prafu\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator CalibratedClassifierCV from version 0.20.2 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your bot is ready to talk! Type your messages here or send 'stop'\n",
      "Analysre here \n",
      "fuck you\n",
      "{'recipient_id': 'default', 'text': 'Please do not use such language'}\n",
      "Analysre here \n",
      "hello\n",
      "Give an entity:\n",
      "/greet\n",
      "{'recipient_id': 'default', 'text': 'Hey! How are you?'}\n",
      "Analysre here \n",
      "idiot\n",
      "{'recipient_id': 'default', 'text': 'Great carry on!'}\n"
     ]
    }
   ],
   "source": [
    "from profanity_check import predict, predict_prob\n",
    "print(\"Your bot is ready to talk! Type your messages here or send 'stop'\")\n",
    "while True:\n",
    "  a = input(\"Analysre here \\n\")\n",
    "  predict([a])\n",
    "  result = predict_prob([a])\n",
    "\n",
    "  if result > 0.2:\n",
    "    a = \"/foul\"\n",
    "    responses = agent.handle_message(a)\n",
    "    for response in responses:\n",
    "     print(response)\n",
    "     \n",
    "  else:\n",
    "    a = input(\"Give an entity:\\n\")\n",
    "    if a == 'stop':\n",
    "     break\n",
    "    responses = agent.handle_message(a)\n",
    "    for response in responses:\n",
    "     print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
